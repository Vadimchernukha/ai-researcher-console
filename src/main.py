# enhanced_main.py
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–ª–∞–≤–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ —Å –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∫–∞—á–µ—Å—Ç–≤–∞
"""

import asyncio
import csv
import logging
from datetime import datetime, timedelta
from playwright.async_api import async_playwright
from tqdm import tqdm
import nest_asyncio
import aiofiles
import time

from src.pipelines.classification_pipeline import EnhancedPipeline
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
import config.settings as config
import src.core.enhanced_utils as utils

nest_asyncio.apply()

class EnhancedProcessor:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é"""
    
    def __init__(self, profile: str | None = None):
        # –ë–µ—Ä—ë–º –ø—Ä–æ—Ñ–∏–ª—å –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–∞, –∑–∞—Ç–µ–º –∏–∑ ENV, –∑–∞—Ç–µ–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        effective_profile = profile or os.environ.get("PROFILE") or getattr(config, "PROFILE", "software")
        self.pipeline = EnhancedPipeline(profile=effective_profile)
        self.processed_domains = set()
        self.setup_logging()
        
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–∫–æ–Ω—Å–æ–ª—å ‚Äî —Ç–∏—Ö–∞—è)"""
        # –°–æ–∑–¥–∞–µ–º –∫–∞—Ç–∞–ª–æ–≥–∏ –¥–ª—è –ª–æ–≥–æ–≤ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        try:
            os.makedirs(os.path.dirname(config.LOG_FILE), exist_ok=True)
            os.makedirs(os.path.dirname(config.DETAILED_LOG_FILE), exist_ok=True)
            os.makedirs(os.path.dirname(config.PASS_2_OUTPUT_FILE), exist_ok=True)
        except Exception:
            pass

        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.ERROR)  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ —Ç–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏
        file_handler_main = logging.FileHandler(config.LOG_FILE, mode='w', encoding='utf-8')
        file_handler_detail = logging.FileHandler(config.DETAILED_LOG_FILE, mode='w', encoding='utf-8')

        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[file_handler_main, file_handler_detail, console_handler]
        )
    
    async def run_enhanced_processing(self, profile: str, input_file: str = "web.csv", progress_callback=None, fail_fast: bool = False):
        """–ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∞–π–ª–æ–≤
        self._prepare_output_files()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        urls_to_process = self._load_urls_to_process(input_file)
        
        if not urls_to_process:
            print("‚úÖ –í—Å–µ —Å–∞–π—Ç—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç")
            return
        
        total_count = len(urls_to_process)
        start_dt = datetime.now()
        print(f"üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ {total_count} —Å–∞–π—Ç–æ–≤ | –°—Ç–∞—Ä—Ç: {start_dt.strftime('%H:%M:%S')}")

        write_lock = asyncio.Lock()
        pbar = tqdm(total=total_count, desc="Progress", unit="site", dynamic_ncols=True, leave=True, bar_format="{l_bar}{bar} {n_fmt}/{total_fmt}")

        total_processed = 0
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        successful_results = []
        
        for i in range(0, len(urls_to_process), config.BATCH_SIZE_FOR_RESTART):
            batch_urls = urls_to_process[i:i + config.BATCH_SIZE_FOR_RESTART]
            
            batch_results = await self._process_batch(batch_urls, write_lock, pbar, profile, fail_fast)
            successful_results.extend(batch_results)
            
            total_processed += len(batch_urls)
            # –°—Ç–∞—Ç–∏—á–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –±–µ–∑ –ø–æ—Å—Ç—Ñ–∏–∫—Å–æ–≤
            if progress_callback:
                progress_callback(total_processed, len(urls_to_process))
            
            # –£–±–∏—Ä–∞–µ–º –ø–∞—É–∑—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        
        pbar.close()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç –±–µ–∑ –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª—å
        self.pipeline.generate_quality_report()
        
        finish_dt = datetime.now()
        print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ –∑–∞ {(finish_dt - start_dt)} | –ù–∞–π–¥–µ–Ω–æ: {len(successful_results)} | –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {config.PASS_2_OUTPUT_FILE}")
    
    async def _process_batch(self, batch_urls: list, write_lock: asyncio.Lock, pbar: tqdm, profile: str, fail_fast: bool) -> list:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ URL"""
        
        successful_results = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--disable-dev-shm-usage', '--no-sandbox']
            )
            
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            
            semaphore = asyncio.Semaphore(config.MAX_CONCURRENT)
            
            async def process_single_website(url):
                async with semaphore:
                    try:
                        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π timeout –Ω–∞ –≤–µ—Å—å —Å–∞–π—Ç - 45s max
                        async with asyncio.timeout(45):
                            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
                            if not url.startswith("http"):
                                url = f"https://{url}"
                                
                            # –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω (–±–µ–∑ –∫–æ–Ω—Å–æ–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞)
                            result = await self.pipeline.process_website_comprehensive(context, url)
                        
                        if result:
                            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                            await self._save_result(result, write_lock)
                            successful_results.append(result)
                            
                    except asyncio.TimeoutError:
                        logging.warning(f"Timeout 45s –¥–ª—è {url}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    except Exception as e:
                        logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
                        if fail_fast:
                            raise
                    finally:
                        pbar.update(1)
            
            # –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            tasks = [process_single_website(url) for url in batch_urls]
            if fail_fast:
                # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –Ω–∞ –ø–µ—Ä–≤–æ–π –æ—à–∏–±–∫–µ
                for t in tasks:
                    try:
                        await t
                    except Exception:
                        await browser.close()
                        raise
            else:
                await asyncio.gather(*tasks)
            
            await browser.close()
        
        return successful_results
    
    async def _save_result(self, result: dict, write_lock: asyncio.Lock):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Ñ–∞–π–ª"""
        
        # –ü–∏—à–µ–º –∫—Ä–∞—Ç–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ (reasoning) —Ä—è–¥–æ–º —Å –º–µ—Ç–∫–æ–π
        row = [result["domain"], result["classification"], result.get("comment", "")] 
        
        async with write_lock:
            async with aiofiles.open(config.PASS_2_OUTPUT_FILE, 'a', encoding='utf-8', newline='') as f:
                row_str = utils.format_csv_row(row) + "\n"
                await f.write(row_str)
    
    def _prepare_output_files(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        os.makedirs(os.path.dirname(config.PASS_2_OUTPUT_FILE), exist_ok=True)
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open(config.PASS_2_OUTPUT_FILE, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["Domain", "Classification", "Comment"])
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        self.processed_domains = utils.get_processed_domains(config.PASS_2_OUTPUT_FILE)
    
    def _load_urls_to_process(self, input_file: str = None) -> list:
        """–ó–∞–≥—Ä—É–∑–∫–∞ URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        file_to_use = input_file or config.INPUT_FILE
        try:
            with open(file_to_use, 'r', encoding='utf-8') as f:
                urls = [row[0].strip() for row in csv.reader(f) if row and row[0].strip()]
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö
                domain_to_url = {}
                for url in urls:
                    domain = url.replace("https://", "").replace("http://", "").split('/')[0].lower()
                    if domain not in self.processed_domains:
                        domain_to_url[domain] = url
                
                return list(domain_to_url.values())
                
        except FileNotFoundError:
            logging.error(f"–§–∞–π–ª {config.INPUT_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return []
    
    def _print_batch_stats(self, results: list, batch_size: int):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –±–∞—Ç—á—É"""
        
        success_rate = (len(results) / batch_size) * 100 if batch_size > 0 else 0
        print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {len(results)}/{batch_size} ({success_rate:.1f}%)")
        
        if results:
            avg_confidence = sum(r["confidence"] for r in results) / len(results)
            avg_time = sum(r["processing_time"] for r in results) / len(results)
            print(f"   üìä –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.1f}%, –≤—Ä–µ–º—è: {avg_time:.1f}—Å")
    
    def _print_final_stats(self, results: list, total_processed: int):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        
        print(f"\nüìà –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   üéØ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_processed}")
        print(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π: {len(results)}")
        print(f"   üìä –û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {(len(results)/total_processed)*100:.1f}%")
        
        if results:
            avg_confidence = sum(r["confidence"] for r in results) / len(results)
            avg_time = sum(r["processing_time"] for r in results) / len(results)
            total_time = sum(r["processing_time"] for r in results)
            
            print(f"   üéØ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.1f}%")
            print(f"   ‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Å–∞–π—Ç: {avg_time:.1f}—Å")
            print(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_time:.1f}—Å")

async def main(profile: str | None = None, input_file: str = None, fail_fast: bool = False):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("="*60)
    print("üéØ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –°–ê–ô–¢–û–í")
    print("="*60)
    
    processor = EnhancedProcessor(profile=profile)
    
    try:
        await processor.run_enhanced_processing(profile or "software", input_file, fail_fast=fail_fast)
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  –ü—Ä–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        logging.exception("Critical error in main")

if __name__ == "__main__":
    asyncio.run(main())
