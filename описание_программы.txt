ОПИСАНИЕ СИСТЕМЫ КЛАССИФИКАЦИИ САЙТОВ
=========================================

ОБЩЕЕ НАЗНАЧЕНИЕ:
Данная программа представляет собой интеллектуальную систему для автоматической классификации и анализа веб-сайтов различных компаний. Система использует технологии веб-скрапинга, искусственный интеллект и многоэтапную валидацию для определения соответствия сайтов заданным критериям поиска.

ОСНОВНЫЕ ВОЗМОЖНОСТИ:
- Автоматическое извлечение контента с веб-сайтов
- ИИ-анализ содержимого сайтов с использованием Gemini API
- Классификация компаний по различным отраслевым профилям
- Многоэтапная валидация результатов
- Мониторинг качества обработки
- Генерация детальных отчетов

ПОДДЕРЖИВАЕМЫЕ ПРОФИЛИ КЛАССИФИКАЦИИ:
1. software - программные продукты и SaaS решения
2. iso - компании с ISO сертификацией
3. telemedicine - телемедицинские сервисы
4. pharma - фармацевтические компании
5. edtech - образовательные технологии
6. marketing - маркетинговые агентства
7. fintech - финансовые технологии
8. healthtech - медицинские технологии
9. elearning - онлайн обучение
10. software_products - продуктовые IT компании
11. salesforce_partner - партнеры Salesforce
12. hubspot_partner - партнеры HubSpot
13. aws - партнеры AWS
14. shopify - партнеры Shopify
15. ai_companies - AI компании
16. mobile_app - мобильные приложения
17. recruiting - рекрутинговые агентства
18. banking - банковские услуги
19. platforms - IT платформы

АРХИТЕКТУРА СИСТЕМЫ:
====================

/src/main.py - главный модуль запуска
/scripts/run_classifier.py - скрипт для запуска с параметрами
/src/pipelines/classification_pipeline.py - основной пайплайн обработки
/src/scrapers/content_scraper.py - модуль извлечения контента
/src/analyzers/ai_analyzer.py - ИИ анализ и классификация
/src/analyzers/extraction_prompts.py - промпты для ИИ
/src/validators/quality_monitor.py - контроль качества
/config/settings.py - настройки системы

ДЕТАЛЬНЫЙ ПРОЦЕСС РАБОТЫ:
=========================

ЭТАП 1: ИЗВЛЕЧЕНИЕ КОНТЕНТА (Content Extraction)
-----------------------------------------------
Цель: Получить текстовое содержимое веб-страницы

Процесс:
- Использует Playwright для загрузки веб-страниц
- Применяет множественные стратегии извлечения контента:
  * Поиск основного контента (main, article, .content)
  * Извлечение из body если основной контент не найден
  * Поиск ключевых секций с бизнес-информацией
  * Сбор всех параграфов как fallback стратегия
- Блокирует загрузку изображений и стилей для ускорения
- Применяет таймауты для предотвращения зависания
- Очищает HTML от служебных элементов (script, style, nav)

Критерии успеха:
- Минимум 20 символов контента
- Наличие бизнес-ключевых слов
- Достаточное количество осмысленных слов (30+)

ЭТАП 2: ВАЛИДАЦИЯ КОНТЕНТА (Content Validation)
----------------------------------------------
Цель: Убедиться что извлеченный контент пригоден для анализа

Процесс:
- Проверка длины контента (минимум 20 символов)
- Подсчет количества слов и предложений
- Поиск бизнес-ключевых слов (services, products, solutions, etc.)
- Расчет общего балла качества контента
- Выявление проблем извлечения

Метрики качества:
- Длина текста
- Количество слов
- Количество предложений
- Наличие бизнес-терминологии
- Общий балл качества (0-100)

ЭТАП 3: ПЕРВИЧНАЯ КЛАССИФИКАЦИЯ (Initial Classification)
-------------------------------------------------------
Цель: Извлечь структурированную информацию о компании

Процесс:
- Отправка контента в Gemini AI с профиль-специфичным промптом
- Извлечение ключевых фактов о компании:
  * Описание деятельности компании
  * Бизнес-модель (Product/Service/SaaS/etc.)
  * Названия продуктов/услуг
  * Целевая аудитория
  * Технические особенности
- Валидация полученных данных
- Повторные попытки при неудаче (до 3 раз)

Извлекаемые данные (зависят от профиля):
- company_description - описание компании
- business_model - модель бизнеса
- software_name - название ПО
- target_audience - целевая аудитория
- has_login_button - наличие авторизации
- has_pricing_page - наличие тарифов

ЭТАП 4: ДЕТАЛЬНЫЙ АНАЛИЗ (Detailed Analysis)
-------------------------------------------
Цель: Глубокий анализ потенциальных совпадений

Процесс (выполняется только для потенциальных совпадений):
- Применение профиль-специфичного классификатора
- Поиск конкретных ключевых слов и паттернов
- Анализ соответствия критериям поиска
- Оценка уверенности в классификации

Логика принятия решений:
- Проверка ключевых слов в описании компании
- Анализ упомянутых продуктов и услуг
- Оценка бизнес-модели
- Поиск отраслевых маркеров

ЭТАП 5: КРОСС-ВАЛИДАЦИЯ (Cross Validation)
-----------------------------------------
Цель: Подтверждение результатов классификации

Процесс:
- Повторный анализ с альтернативными промптами
- Сравнение результатов разных подходов
- Проверка консистентности данных
- Выявление противоречий в классификации

Критерии валидации:
- Согласованность результатов
- Достаточность доказательств
- Отсутствие конфликтующих данных

ЭТАП 6: ФИНАЛЬНОЕ РЕШЕНИЕ (Final Decision)
-----------------------------------------
Цель: Принятие окончательного решения о классификации

Процесс:
- Агрегация всех собранных данных
- Расчет итогового балла уверенности
- Принятие бинарного решения (Match/No Match)
- Формирование обоснования решения

Результат:
- classification - итоговая классификация
- confidence - уровень уверенности (0-100)
- reasoning - обоснование решения
- is_match - финальный результат (true/false)

СИСТЕМА КАЧЕСТВА И МОНИТОРИНГА:
===============================

Контроль качества осуществляется на каждом этапе:

1. МОНИТОРИНГ ИЗВЛЕЧЕНИЯ:
   - Время обработки каждого сайта
   - Успешность извлечения контента
   - Качество полученного текста

2. ВАЛИДАЦИЯ ИИ-ОТВЕТОВ:
   - Проверка формата JSON
   - Валидация обязательных полей
   - Проверка логической согласованности

3. ОТЧЕТНОСТЬ:
   - Детальные логи обработки
   - Статистика успешности по этапам
   - Анализ типичных ошибок
   - Рекомендации по улучшению

ФАЙЛЫ РЕЗУЛЬТАТОВ:
==================

processing.log - основной лог обработки
detailed_processing.log - детальная информация
quality_report.csv - отчет о качестве
results_final_clean.csv - итоговые результаты

РЕЖИМЫ ЗАПУСКА:
===============

1. Основной запуск:
   python src/main.py

2. С указанием профиля:
   python scripts/run_classifier.py --profile software --input data/domains.csv

3. Быстрый останов при ошибках:
   python scripts/run_classifier.py --fail-fast

ТЕХНИЧЕСКИЕ ОСОБЕННОСТИ:
=========================

- Асинхронная обработка для высокой производительности
- Защита от зависания с таймаутами
- Автоматические повторы при временных сбоях
- Блокировка ненужных ресурсов для ускорения
- Интеллектуальная балансировка нагрузки
- Подробное логирование всех операций

ВХОДНЫЕ ДАННЫЕ:
===============

CSV файл с доменами в формате:
domain
example.com
another-site.com

ВЫХОДНЫЕ ДАННЫЕ:
================

CSV файл с результатами классификации:
- domain - обработанный домен
- classification - результат классификации
- comment - обоснование решения
- confidence - уровень уверенности
- processing_time - время обработки

ПРЕИМУЩЕСТВА СИСТЕМЫ:
====================

1. Высокая точность благодаря многоэтапной проверке
2. Масштабируемость для обработки больших объемов
3. Гибкость настройки под разные отрасли
4. Детальная отчетность и мониторинг
5. Устойчивость к сбоям и таймаутам
6. Автоматическая оптимизация производительности

ОГРАНИЧЕНИЯ:
============

- Зависимость от доступности внешних сайтов
- Требует подключение к Gemini API
- Скорость ограничена лимитами API
- Качество зависит от содержимого сайтов
- Необходимо периодическое обновление промптов
